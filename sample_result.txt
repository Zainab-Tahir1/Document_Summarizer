Sample Results:

Document: ptcl_general_privacy_policy.txt

Query: Summmarize the General policies of ptcl mentioned in the document

Show retrieved chunk 1
Transactions and usage details, such as logs of voice/data and value-added services, location data, and subscription and billing of our products and services; iii. Response data related your request for service or assistance; and iv. Any other information that you have provided to PTCL Group, authorized business partners and third parties including PTCL Group’s subsidiaries, affiliates, and associated companies for subscribing and/or continue using our service(s)/ product(s). 2. Your information will primarily be stored in electronic form. However, certain data can also be stored in physical form. We will ensure to implement adequate organizational and technical measures to protect your information and privacy.

Show retrieved chunk 2
Changes to this Notice 15.The Notice should be read in conjunction with the general terms and conditions (e.g. General Terms and Conditions applicable at the time of SIM Sale/Purchase through BVS) as well as the other products/services and app(s)/website(s) specific terms and conditions and privacy policies. PTCL Group has the discretion to update this Notice at any time based on the changes in business, legal and regulatory requirement and made available to you online. PTCL Group companies will make efforts to communicate about significant changes to this Notice from time to time through any suitable mode such as Web-ticker or In-App notification or SMS or email or social media/digital engagement platforms/Apps. 16. You acknowledge and agree that it is your responsibility to review this Notice periodically and become aware of modifications. Hence, you are requested to visit PTCL Group companies’ relevant Website/Apps on a regular basis for updates and changes in privacy related provisions. Contact Information 17. If you have a question, concern or complaint about this privacy Notice or handling of your information or how we are processing your personal data, you can access PTCL Group’s relevant Website/Apps for contacting us.

Show retrieved chunk 3
Customer Consent “By sharing your information and/or subscribing and/or continue using our service(s)/ product(s)”, including (but not limited to) in writing, via click-through agreements, verbally, interactive voice responses, digital engagement’s through App(s) of PTCL Group, PTCL Group brands and PTCL Group presence on social media e.g. PTCL Group WhatsApp number, Facebook page etc., which among others, may be available on PTCL Group Website(s), PTCL Group app(s), or otherwise, pursuant to which we provide you PTCL Group products/services; “You acknowledge and consent (including but not limited to cookies) that PTCL Group may responsibly collect and use your information as well as share/receive your information and analytical data with/from its authorized business partners and third parties including PTCL’s subsidiaries, affiliates & associated companies and cloud-based authorized business partners” for the commercial purpose of understanding, designing innovative products and enhancing your experience as a user of PTCL Group app(s), website(s) and products/services etc. Collection and Storing of Information
To provide you with the best possible services, PTCL Group, authorized business partners and third parties including PTCL Group’s subsidiaries, affiliates, and associated companies (e.g. franchises, managed service providers and authorized data processors and controllers etc.), would responsibly collect and store your information for a number of legal, regulatory and business requirements. The collection may include information pertaining to: i. Verification of your identity, including your Name, Social Media ID, CNIC, Date of Birth, Address, and other details such as Phone Number and Email when you purchase a PTCL Group’s Mobile SIM card, connection, port-in to PTCL Group network, subscribe and use PTCL Group products/services/websites or otherwise interact with PTCL Group in any manner.

Show retrieved chunk 4
Personal Information “Personal Information” may be the information/data that relates to you as a Customer/Subscriber and could be used to reasonably identify or locate subscriber; however, this data when anonymized/pseudonymized or converted in analytical data, which is incapable of identifying you as a person, is not considered the personal information. For the purpose of clarity, personal information may include combination of your name, father/mother name, address, photo,/logo, CNIC Number/Passport Number/Registration Number, CDRs, IP Address details, Social Media Accounts, Phone Number, Webiste, NTN Number, PIN Code, email address and details received from Government entities for SIM sale issuance and activation process; however, this personal information when anonymized/pseudonymized or converted in analytical data, which is incapable of identifying you as a person, is not considered the personal information.
Sensitive Personal Information For the purposes of this Notice, PTCL Group would treat following information as sensitive personal data in accordance with the applicable regulatory regime and terms of the engagement: (i) Passwords, (ii) financial data (bank account, credit card, debit card, or other payment instruments), (iii) health data (physical, behavioral, psychological, and mental health conditions or medical record), (iv) computerized national identity card or passport, (v) biometric data, (vi) genetic data, (vii) religious beliefs, (viii) criminal records, (ix) political affiliations, (x) caste or tribe, (xi) individual’s ethnicity, (xii) physical identifiable current location.

Show retrieved chunk 5
Non-Personal Information In order to provide you quality services, PTCL Group, authorized business partners and third parties including PTCL’s subsidiaries, affiliates and associated companies may also collect, use and share certain information that are not meant to directly identify you as an individual. The non-personal information may include but not limited to connectivity sessions, serving tower location, web beacons, internet protocols (IP), operating system (OS), Web Browsers, aggregate logs and analytical data.

Summary:

This document outlines the terms and conditions for using services and products offered by PTCL Group, a telecommunications company. Here's a summary:

What's Included:

Information about transactions, usage, and subscription details
Response data related to your requests for service or assistance
Any other information provided to PTCL Group, authorized partners, and third parties

How PTCL Group Collects and Stores Your Information:
Your personal information, including name, address, phone number, email, and other details, will be collected and stored electronically.
Some information, like your identity verification and payment details, will be stored in physical form (e.g., SIM card).

What's Considered Personal and Sensitive Information:
Passwords
Financial data
Health data
Computerized national identity cards
Biometric data
Genetic data
Religious beliefs
Criminal records
Political affiliations
Caste or tribe
Individual's ethnicity

What's Not Personal Information:
Connectivity sessions
Web beacons
Internet protocols
Operating system
Web browser
Aggregate logs
Analytical data

Changes to the Notice:
PTCL Group reserves the right to update this notice at any time based on business, legal, and regulatory requirements.

How to Contact Us:
You can find contact information for PTCL Group on their website.

---------------------------------------------------------------------------------------------------------

Document: 1953_L_1.txt

Query: Summarize the Document, tell me what the case is all about.

Show retrieved chunk 1
these remarks do not throw much light on the point before us. the last decision referred to was the decision of the privy council in mahomed shumsool v shewukram 1874 indlaw pc 12 1874 75 2 i a 7 there a hindu inhabitant of bihar by a document of a testamentary character declared his daughter who had two daughters as his heir and after her two daughters together with their children were declared heirs and malik. one daughter of the daughter predeceased the testator without issue and the other daughter died after the death of the testator leaving an only son the respondent in that case. in a suit by the respondent against his grandmother the daughter of the testator for a declaratory order preserving unmolested his future right and title to the said lands it was held that the daughter took an estate subject to her daughters succeeding her. in this judgment the following observations were emphasized as relevant to this enquiry it has been contended that these latter expressions qualify the generality of the former expressions and that the will taken as a whole must be construed as intimating the intention of the testator that mst. rani dhun kaur should not take an absolute estate but that she should be succeeded in her estate by her two daughters. in other words that she should take an estate very much like the ordinary estate of a hindu widow. in construing the will of a hindu it is not improper to take into consideration what are known to be the ordinary notions and wishes of hindus with respect to the devolution of property. it may be assumed that a hindu generally desires that an estate especially an ancestral estate shall be retained in his family and it may be assumed that a hindu knows that as a general rule at all events women do not take absolute estates of inheritance which they are enabled to alienate.

Show retrieved chunk 2
in other words to ascertain his wishes by putting itself so to say in his armchair. considering the will in the light of these principles it seems to us that lakshminarayan iyer intended by his will to direct that his entire properties should be enjoyed by his widow during her lifetime but her interest in these properties should come to an end on her death that all these properties in their entirety should thereafter be enjoyed as absolute owners by his daughter and her heirs with powers of alienation gift exchange and sale from generation to generation. he wished to make his daughter a fresh stock of descent so that her issue male or female may have the benefit of his property. they were the real persons whom he earmarked with certainty as the ultimate recipients of his bounty. in express terms he conferred on his daughter powers of alienation byway of gift exchange sale but in sharp contrast to this on his widow he conferred no such powers. the direction to her was that she should enjoy the entire properties including the outstandings etc. and these shall thereafter pass to her daughters. though no restraint in express terms was put on her powers of alienation in case of necessity even that limited power was not given to her in express terms. if the testator had before his mind 's eye his daughter and her heirs as the ultimate beneficiaries of his bounty that intention could only be achieved by giving to the widow a limited estate because by conferring a full hindu widow 's estate on her the daughter will only have a mere spes successions under the hindu law which may or may not mature and under the will her interest would only be a contingent one in what was left indisposed of by the widow. it is significant that the testator did not say in the will that the daughter will enjoy only the properties left indisposed of by the widow. the extent of the grant so far as the properties mentioned in the schedule are concerned to the daughter and the widow is the same.

Show retrieved chunk 3
but there were other indications in that will showing that a widow 's estate had been given. the fact that the gift over was a contingent bequest was by itself taken as a sure indication that the preceding bequest was that of a widow 's estate. there is no such indication in the will before us. reliance was next placed on the decision in pavani subbamma v ammala rama naidu 1937. 1 m l j 268. 1936 indlaw mad 236. under the will there dealt with the widow s was to enjoy the properties and after her lifetime the properties were to be taken in the ratio of three to five by the son 's daughter and the daughter 's son respectively. a suit was instituted by the son 's daughter for the recovery of possession of her share in one item of property forming part of the estate which had been sold by section the question for decision in that case was whether section was at all entitled to sell anything more than her life interest even for purposes of meeting a necessity binding upon the estate. varadachari j held that since in the will the gift over to the grand children was of the entire properties and not a mere gift by way of defeasance it had to be held that it indicated that the prior gift in favour of the widow was only of a limited interest. this decision therefore goes against the contention of the learned counsel but he placed reliance on the observations made in the judgment when the learned judge proceeded to say in deference to the view taken in maharaja of kolhapur v sundaram iyer 1925 i l r 48 mad 1 it may be possible to create an interest analogous to a woman 's estate in hindu law notwithstanding the addition of a gift over and that the estate taken by section need not necessarily be only a life estate in the english law sense of the term. we do not understand how such passing observations can be helpful in deciding the present case.

Show retrieved chunk 4
he omits the words during her life with reference to the disposition in favour of the daughter. the words pass to my daughter would rather indicate that in the ordinary course of devolution the estate should pass to her that is the daughter and then to the grandsons. the words used in favour of the grandsons seem to indicate that the estate conferred on the daughter was not a life estate because there is no direct gift in favour of the grandsons but on the other hand what he says is that through his daughter the estate shall pass to his grandsons. either he must have intended that the daughter should convey the property either by will or inter vivos to the grandsons or she having taken the estate through her it should pass to the grandsons in the ordinary course of devolution. if it was the daughter 's estate that was intended to be conferred there can be no question that the estate taken by the grandsons is not a vested interest. this line of reasoning which appealed to the learned judges is not of much he to us here as the language hi this will is quite different. if the same line of reasoning is adopted here the decision of the case would go against the client of mr k section k iyengar because in the will in this case the widow 's estate is delimited by the words till your lifetime. reliance was next placed on maharaja of kolhapur v sundaram iyer 1925 i l r 48 mad. that was a case of a government grant on the special terms set out therein and the question arose as to the nature of the grant. there it was said that the widows of sivaji raja got the gift of a life estate very much resembling the ordinary estate of a hindu widow and with all the incidents of a widow 's estate except the liability to be divested but nevertheless a life estate rather than an estate of inheritance. these remarks do not throw much light on the point before us.

Show retrieved chunk 5
we do not understand how such passing observations can be helpful in deciding the present case. assuming that it is possible to create a hindu woman 's estate not with standing the addition of a gift over the question nevertheless whether that had been done in a given case must depend on the terms of the particular instrument under consideration. the following remarks in the privy council decision in nathu ram mahajan v gangayabai 1938. 2 m l j 562 were next cited as the will gave her the right to enjoy the income of the estate during her lifetime it was evidently contemplated that she should as provided by the hindu law in the case of a widow be in possession of the estate. such casual observation made in respect of a will couched in entirely different terms can not afford much assistance in the decision of the case. in vasantharao ammannamma v venkata kodanda rao pantalu 1939. indlaw mad 621 1940. m l j 188 the next case cited a hindu testator who was a retired subordinate judge provided by his will as follows out of the aforestated ancestral lands the oneninth share to which i am entitled shall be enjoyed after my death by my wife till her death and after her death it shall pass to section son of my second elder brother deceased. my self acquired properties shall on my death be enjoyed by my wife till her death and after her death they shall pass to my daughter. thereafter they shall pass to my grandson through my daughter. the will was construed as giving the self acquired properties ultimately to the grandsons and the estate of the daughter was likened to an estate which she would take under the law of inheritance that is a limited estate analogous to a widow 's estate. at page 193 of the report it was observed as follows the question therefore arises did he intend to confer only a life estate or a daughter 's estate. it seems to us that he meant to give a daughter 's estate rather than a life estate. he omits the words during her life with reference to the disposition in favour of the daughter.


Final Summary:
The document discusses the interpretation of a will from a Hindu testator, Ramanujan Iyer, and its implications on the distribution of property in his estate. The testator's will stated that his entire properties should be enjoyed by his widow during her lifetime, but after her death, all the properties should be enjoyed by his daughter and her heirs with powers of alienation, gift exchange, and sale.

The document mentions several decisions and judgments that have been cited as relevant to this enquiry, including Mahomed Shumsool v Shewukram (1874), Rani Dhan Kaur v Pavani Subbamma (1937), Maharaja of Kolhapur v Sundaram Iyer (1925), Nathu Ram Mahajan v Gangayabai (1938), and Vasantharao Ammannamma v Venkata Kodanda Rao Pantalu (1939).

The document argues that the testator's intention is to give his widow a limited estate, which can be passed on to his daughter and her heirs, but not a full hindu widow's estate. It suggests that the testator intended to confer powers of alienation, gift exchange, and sale on his daughter, and that the estate should pass to her after her death.

However, the document notes that the observations made in these decisions and judgments are not helpful in deciding the case, and that the language of the testator's will is quite different. It also mentions that assuming the existence


---------------------------------------------------------------------------------------------------------

Document: attention_all_you_need

Query: Summarize the main innovation introduced in the 'Attention Is All You Need' paper and explain how it differs from previous sequence-to-sequence models.

Show retrieved chunk 1
Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and

Show retrieved chunk 2
used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is

Show retrieved chunk 3
ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6 length nis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [ 31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.

Show retrieved chunk 4
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is

Show retrieved chunk 5
the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning rate over the course of training, according to the formula: lrate =d−0.5 model·min(step_num−0.5, step _num·warmup _steps−1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 . 5.4 Regularization We employ three types of regularization during training: 7

Summary: 
The paper "Attention Is All You Need" is a groundbreaking work in sequence-to-sequence models that has revolutionized the field of natural language processing. The authors, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, propose a new network architecture called the Transformer.

The Transformer is based solely on attention mechanisms, which are a type of neural network that enables the model to attend to different parts of the input sequence and weigh their importance. This is in contrast to traditional sequence-to-sequence models, which use recurrent or convolutional neural networks to connect the encoder and decoder.

The key innovations of the Transformer include:

Simple and efficient architecture: The Transformer is a stack of identical layers, each consisting of a multi-head self-attention mechanism and a point-wise fully connected layer. This makes it much simpler and more efficient than traditional sequence-to-sequence models.
No recurrence or convolution: The Transformer does not use recurrence or convolutional layers, which are typically used in traditional sequence-to-sequence models. Instead, it uses self-attention to connect the encoder and decoder.
Improved parallelization: The Transformer is designed to be more parallelizable, which makes it faster to train and use.
The authors demonstrate the effectiveness of the Transformer on several tasks, including: Machine Translation etc.